{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LoRA implementation with PyTorch\n\nLet's start by importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets \nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:25:33.671077Z","iopub.execute_input":"2024-05-14T10:25:33.671561Z","iopub.status.idle":"2024-05-14T10:25:33.678319Z","shell.execute_reply.started":"2024-05-14T10:25:33.671512Z","shell.execute_reply":"2024-05-14T10:25:33.677132Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Make the model deterministic","metadata":{}},{"cell_type":"code","source":"# Make torch deterministic\n_ = torch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:25:33.680390Z","iopub.execute_input":"2024-05-14T10:25:33.680849Z","iopub.status.idle":"2024-05-14T10:25:33.693939Z","shell.execute_reply.started":"2024-05-14T10:25:33.680812Z","shell.execute_reply":"2024-05-14T10:25:33.692423Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"We will be training a network to classify MNIST digits and then fine-tune the network on a particular digit on which it doesn't perform well.","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n# Load the MNIST dataset\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Load the MNIST test set\nmnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n\n# Define the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:25:33.695411Z","iopub.execute_input":"2024-05-14T10:25:33.695777Z","iopub.status.idle":"2024-05-14T10:25:33.826310Z","shell.execute_reply.started":"2024-05-14T10:25:33.695713Z","shell.execute_reply":"2024-05-14T10:25:33.824975Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Create the Neural Network to classify the digits, making it overly complicated to better show the power of LoRA","metadata":{}},{"cell_type":"code","source":"# Create an overly expensive neural network to classify MNIST digits\n# Daddy got money, so I don't care about efficiency\nclass RichBoyNet(nn.Module):\n    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n        super(RichBoyNet,self).__init__()\n        self.linear1 = nn.Linear(28*28, hidden_size_1) \n        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n        self.linear3 = nn.Linear(hidden_size_2, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, img):\n        x = img.view(-1, 28*28)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n\nnet = RichBoyNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:25:33.831148Z","iopub.execute_input":"2024-05-14T10:25:33.831786Z","iopub.status.idle":"2024-05-14T10:25:33.876051Z","shell.execute_reply.started":"2024-05-14T10:25:33.831737Z","shell.execute_reply":"2024-05-14T10:25:33.874605Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Train the network only for 1 epoch to simulate a complete general pre-training on the data","metadata":{}},{"cell_type":"code","source":"def train(train_loader, net, epochs=5, total_iterations_limit=None):\n    cross_el = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\n    total_iterations = 0\n\n    for epoch in range(epochs):\n        net.train()\n\n        loss_sum = 0\n        num_iterations = 0\n\n        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n        if total_iterations_limit is not None:\n            data_iterator.total = total_iterations_limit\n        for data in data_iterator:\n            num_iterations += 1\n            total_iterations += 1\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            output = net(x.view(-1, 28*28))\n            loss = cross_el(output, y)\n            loss_sum += loss.item()\n            avg_loss = loss_sum / num_iterations\n            data_iterator.set_postfix(loss=avg_loss)\n            loss.backward()\n            optimizer.step()\n\n            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n                return\n\ntrain(train_loader, net, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:25:33.878375Z","iopub.execute_input":"2024-05-14T10:25:33.878912Z","iopub.status.idle":"2024-05-14T10:30:00.958344Z","shell.execute_reply.started":"2024-05-14T10:25:33.878868Z","shell.execute_reply":"2024-05-14T10:30:00.957054Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 6000/6000 [04:27<00:00, 22.47it/s, loss=0.236]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Keep a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights","metadata":{}},{"cell_type":"code","source":"original_weights = {}\nfor name, param in net.named_parameters():\n    original_weights[name] = param.clone().detach()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:00.960799Z","iopub.execute_input":"2024-05-14T10:30:00.961920Z","iopub.status.idle":"2024-05-14T10:30:00.974991Z","shell.execute_reply.started":"2024-05-14T10:30:00.961859Z","shell.execute_reply":"2024-05-14T10:30:00.973093Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The the performance of the pretrained network.\nAs we can see, the network performs poorly on the digit 9. Let's fine-tune it on the digit 9","metadata":{}},{"cell_type":"code","source":"def test():\n    correct = 0\n    total = 0\n\n    wrong_counts = [0 for i in range(10)]\n\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc='Testing'):\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            output = net(x.view(-1, 784))\n            for idx, i in enumerate(output):\n                if torch.argmax(i) == y[idx]:\n                    correct +=1\n                else:\n                    wrong_counts[y[idx]] +=1\n                total +=1\n    print(f'Accuracy: {round(correct/total, 3)}')\n    for i in range(len(wrong_counts)):\n        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:00.977063Z","iopub.execute_input":"2024-05-14T10:30:00.977501Z","iopub.status.idle":"2024-05-14T10:30:05.602008Z","shell.execute_reply.started":"2024-05-14T10:30:00.977457Z","shell.execute_reply":"2024-05-14T10:30:05.600885Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:04<00:00, 217.25it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.954\nwrong counts for the digit 0: 34\nwrong counts for the digit 1: 20\nwrong counts for the digit 2: 29\nwrong counts for the digit 3: 98\nwrong counts for the digit 4: 22\nwrong counts for the digit 5: 40\nwrong counts for the digit 6: 58\nwrong counts for the digit 7: 50\nwrong counts for the digit 8: 9\nwrong counts for the digit 9: 96\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's visualize how many parameters are in the original network, before introducing the LoRA matrices.","metadata":{}},{"cell_type":"code","source":"# Print the size of the weights matrices of the network\n# Save the count of the total number of parameters\ntotal_parameters_original = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\nprint(f'Total number of parameters: {total_parameters_original:,}')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:05.603758Z","iopub.execute_input":"2024-05-14T10:30:05.604231Z","iopub.status.idle":"2024-05-14T10:30:05.613189Z","shell.execute_reply.started":"2024-05-14T10:30:05.604190Z","shell.execute_reply":"2024-05-14T10:30:05.611770Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\nTotal number of parameters: 2,807,010\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the LoRA parameterization as described in the paper.\nThe full detail on how PyTorch parameterizations work is here: https://pytorch.org/tutorials/intermediate/parametrizations.html","metadata":{}},{"cell_type":"code","source":"class LoRAParametrization(nn.Module):\n    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n        super().__init__()\n        # Section 4.1 of the paper: \n        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n        nn.init.normal_(self.lora_A, mean=0, std=1)\n        \n        # Section 4.1 of the paper: \n        #   We then scale ∆Wx by α/r , where α is a constant in r. \n        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n        #   As a result, we simply set α to the first r we try and do not tune it. \n        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n        self.scale = alpha / rank\n        self.enabled = True\n\n    def forward(self, original_weights):\n        if self.enabled:\n            # Return W + (B*A)*scale\n            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n        else:\n            return original_weights","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:05.614994Z","iopub.execute_input":"2024-05-14T10:30:05.615400Z","iopub.status.idle":"2024-05-14T10:30:05.627903Z","shell.execute_reply.started":"2024-05-14T10:30:05.615362Z","shell.execute_reply":"2024-05-14T10:30:05.626444Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Add the parameterization to our network.","metadata":{}},{"cell_type":"code","source":"import torch.nn.utils.parametrize as parametrize\n\ndef linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n    # Only add the parameterization to the weight matrix, ignore the Bias\n\n    # From section 4.2 of the paper:\n    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n    #   [...]\n    #   We leave the empirical investigation of [...], and biases to a future work.\n    \n    features_in, features_out = layer.weight.shape\n    return LoRAParametrization(\n        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n    )\n\nparametrize.register_parametrization(\n    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n)\nparametrize.register_parametrization(\n    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n)\nparametrize.register_parametrization(\n    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n)\n\n\ndef enable_disable_lora(enabled=True):\n    for layer in [net.linear1, net.linear2, net.linear3]:\n        layer.parametrizations[\"weight\"][0].enabled = enabled","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:05.632432Z","iopub.execute_input":"2024-05-14T10:30:05.633099Z","iopub.status.idle":"2024-05-14T10:30:05.657993Z","shell.execute_reply.started":"2024-05-14T10:30:05.633056Z","shell.execute_reply":"2024-05-14T10:30:05.656928Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Display the number of parameters added by LoRA.","metadata":{}},{"cell_type":"code","source":"total_parameters_lora = 0\ntotal_parameters_non_lora = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n    print(\n        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n    )\n# The non-LoRA parameters count must match the original network\nassert total_parameters_non_lora == total_parameters_original\nprint(f'Total number of parameters (original): {total_parameters_non_lora:,}')\nprint(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\nprint(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\nparameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\nprint(f'Parameters incremment: {parameters_incremment:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:05.659478Z","iopub.execute_input":"2024-05-14T10:30:05.659908Z","iopub.status.idle":"2024-05-14T10:30:05.692286Z","shell.execute_reply.started":"2024-05-14T10:30:05.659868Z","shell.execute_reply":"2024-05-14T10:30:05.691197Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\nTotal number of parameters (original): 2,807,010\nTotal number of parameters (original + LoRA): 2,813,804\nParameters introduced by LoRA: 6,794\nParameters incremment: 0.242%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 9 and only for 100 batches.","metadata":{}},{"cell_type":"code","source":"# Freeze the non-Lora parameters\nfor name, param in net.named_parameters():\n    if 'lora' not in name:\n        print(f'Freezing non-LoRA parameter {name}')\n        param.requires_grad = False\n\n# Load the MNIST dataset again, by keeping only the digit 9\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\nexclude_indices = mnist_trainset.targets == 9\nmnist_trainset.data = mnist_trainset.data[exclude_indices]\nmnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\ntrain(train_loader, net, epochs=1, total_iterations_limit=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:05.693723Z","iopub.execute_input":"2024-05-14T10:30:05.694718Z","iopub.status.idle":"2024-05-14T10:30:08.008619Z","shell.execute_reply.started":"2024-05-14T10:30:05.694677Z","shell.execute_reply":"2024-05-14T10:30:08.007579Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Freezing non-LoRA parameter linear1.bias\nFreezing non-LoRA parameter linear1.parametrizations.weight.original\nFreezing non-LoRA parameter linear2.bias\nFreezing non-LoRA parameter linear2.parametrizations.weight.original\nFreezing non-LoRA parameter linear3.bias\nFreezing non-LoRA parameter linear3.parametrizations.weight.original\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  99%|█████████▉| 99/100 [00:02<00:00, 45.05it/s, loss=0.0658]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA.","metadata":{}},{"cell_type":"code","source":"# Check that the frozen parameters are still unchanged by the finetuning\nassert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\nassert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\nassert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n\nenable_disable_lora(enabled=True)\n# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n# The original weights have been moved to net.linear1.parametrizations.weight.original\n# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\nassert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n\nenable_disable_lora(enabled=False)\n# If we disable LoRA, the linear1.weight is the original one\nassert torch.equal(net.linear1.weight, original_weights['linear1.weight'])","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:08.010090Z","iopub.execute_input":"2024-05-14T10:30:08.010412Z","iopub.status.idle":"2024-05-14T10:30:08.027727Z","shell.execute_reply.started":"2024-05-14T10:30:08.010384Z","shell.execute_reply":"2024-05-14T10:30:08.026493Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Test the network with LoRA enabled (the digit 9 should be classified better)","metadata":{}},{"cell_type":"code","source":"# Test with LoRA enabled\nenable_disable_lora(enabled=True)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:08.029183Z","iopub.execute_input":"2024-05-14T10:30:08.029637Z","iopub.status.idle":"2024-05-14T10:30:16.314120Z","shell.execute_reply.started":"2024-05-14T10:30:08.029596Z","shell.execute_reply":"2024-05-14T10:30:16.313073Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:08<00:00, 120.85it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.928\nwrong counts for the digit 0: 136\nwrong counts for the digit 1: 18\nwrong counts for the digit 2: 43\nwrong counts for the digit 3: 141\nwrong counts for the digit 4: 115\nwrong counts for the digit 5: 52\nwrong counts for the digit 6: 72\nwrong counts for the digit 7: 90\nwrong counts for the digit 8: 33\nwrong counts for the digit 9: 19\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)","metadata":{}},{"cell_type":"code","source":"# Test with LoRA disabled\nenable_disable_lora(enabled=False)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:30:16.315849Z","iopub.execute_input":"2024-05-14T10:30:16.316187Z","iopub.status.idle":"2024-05-14T10:30:20.774962Z","shell.execute_reply.started":"2024-05-14T10:30:16.316158Z","shell.execute_reply":"2024-05-14T10:30:20.773758Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:04<00:00, 224.81it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.954\nwrong counts for the digit 0: 34\nwrong counts for the digit 1: 20\nwrong counts for the digit 2: 29\nwrong counts for the digit 3: 98\nwrong counts for the digit 4: 22\nwrong counts for the digit 5: 40\nwrong counts for the digit 6: 58\nwrong counts for the digit 7: 50\nwrong counts for the digit 8: 9\nwrong counts for the digit 9: 96\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}